#!/usr/bin/env python3
"""
å®Œæ•´è§£æ attlib.datï¼Œæå–æ‰€æœ‰ Noun ç±»å‹å’Œå±‚çº§å…³ç³»

ä¸ä¾èµ– all_attr_info.jsonï¼Œç›´æ¥ä»äºŒè¿›åˆ¶æ–‡ä»¶è§£æ
åŸºäº IDA Pro åç¼–è¯‘çš„ core.dll åŠ è½½é€»è¾‘

ä½œè€…: AI Analysis Tool
æ—¥æœŸ: 2025
"""

import struct
import json
from pathlib import Path
from typing import Dict, List, Tuple, Set
from collections import defaultdict

class AttlibCompleteParser:
    """å®Œæ•´è§£æ attlib.dat æ–‡ä»¶"""
    
    def __init__(self, attlib_path: str):
        self.attlib_path = Path(attlib_path)
        self.page_size = 2048  # FHDBRN é¡µå¤§å°
        self.words_per_page = 512  # æ¯é¡µ 512 ä¸ª 32 ä½å­—
        
        # å­˜å‚¨è§£æç»“æœ
        self.section_pointers = []  # æ®µæŒ‡é’ˆ
        self.noun_definitions = {}  # hash -> noun_data
        self.attribute_index = {}   # attr_hash -> (record_num, slot_offset)
        self.attribute_definitions = {}  # attr_hash -> attr_data
        self.noun_hierarchy = defaultdict(set)  # parent_noun -> set(child_nouns)
        
    def read_file_header(self, file) -> Dict:
        """è¯»å–æ–‡ä»¶å¤´"""
        file.seek(0)
        
        # è¯»å– UTF-16LE ç¼–ç çš„æ–‡ä»¶æ ‡è¯†
        header_data = file.read(0x100)
        
        try:
            # å°è¯•è§£ç æ–‡ä»¶å¤´
            header_str = header_data[:50].decode('utf-16le', errors='ignore')
            print(f"æ–‡ä»¶å¤´: {header_str[:50]}")
        except:
            pass
        
        return {"header_size": 0x100}
    
    def read_section_pointers(self, file) -> List[int]:
        """è¯»å–æ®µæŒ‡é’ˆè¡¨ (offset 0x0800)"""
        file.seek(0x0800)
        pointers = []
        
        for i in range(8):
            ptr_bytes = file.read(4)
            if len(ptr_bytes) < 4:
                break
            ptr = struct.unpack('>I', ptr_bytes)[0]  # å¤§ç«¯åº
            page_num = ptr // self.page_size
            pointers.append(page_num)
            print(f"  æ®µ {i+1}: é¡µå· {page_num} (åç§» 0x{ptr:08x})")
        
        return pointers
    
    def read_page(self, file, page_num: int) -> List[int]:
        """è¯»å–æŒ‡å®šé¡µçš„ 512 ä¸ª 32 ä½å­—"""
        offset = page_num * self.page_size
        file.seek(offset)
        data = file.read(self.page_size)
        
        if len(data) < self.page_size:
            return []
        
        words = []
        for i in range(self.words_per_page):
            word_bytes = data[i*4:(i+1)*4]
            word = struct.unpack('>I', word_bytes)[0]  # å¤§ç«¯åº
            words.append(word)
        
        return words
    
    def parse_section(self, file, start_page: int, section_name: str) -> List[int]:
        """è§£æä¸€ä¸ªå®Œæ•´çš„æ•°æ®æ®µ"""
        print(f"\nğŸ“– è§£ææ®µ: {section_name} (èµ·å§‹é¡µ: {start_page})")
        
        all_words = []
        page_num = start_page
        total_pages = 0
        
        while total_pages < 1000:  # æœ€å¤šè¯»å– 1000 é¡µé˜²æ­¢æ­»å¾ªç¯
            words = self.read_page(file, page_num)
            if not words:
                break
            
            total_pages += 1
            
            for word in words:
                if word == 0xFFFFFFFF:  # æ®µç»“æŸæ ‡è®°
                    print(f"  âœ“ æ®µç»“æŸæ ‡è®°ï¼Œå…± {total_pages} é¡µï¼Œ{len(all_words)} ä¸ªå­—")
                    return all_words
                elif word == 0x00000000:  # é¡µåˆ‡æ¢æ ‡è®°
                    page_num += 1
                    break
                else:
                    all_words.append(word)
        
        print(f"  âš ï¸ æœªæ‰¾åˆ°æ®µç»“æŸæ ‡è®°ï¼Œè¯»å– {total_pages} é¡µ")
        return all_words
    
    def decode_27_base(self, encoded_words: List[int]) -> str:
        """è§£ç  27 è¿›åˆ¶ç¼–ç çš„æ–‡æœ¬"""
        result = []
        base27_chars = " ABCDEFGHIJKLMNOPQRSTUVWXYZ"
        
        for word in encoded_words:
            chars = []
            temp = word
            for _ in range(6):  # æ¯ä¸ª 32 ä½å­—æœ€å¤š 6 ä¸ªå­—ç¬¦
                if temp == 0:
                    break
                chars.append(base27_chars[temp % 27])
                temp //= 27
            result.extend(reversed(chars))
        
        return ''.join(result).strip()
    
    def parse_atgtix_section(self, words: List[int]):
        """è§£æ ATGTIX å±æ€§ç´¢å¼•æ®µ"""
        print("\nğŸ” è§£æ ATGTIX (å±æ€§ç´¢å¼•æ®µ)...")
        
        i = 0
        attr_count = 0
        min_hash = 531442
        max_hash = 387951929
        
        while i < len(words) - 1:
            attr_hash = words[i]
            i += 1
            
            # èŒƒå›´æ£€æŸ¥
            if attr_hash < min_hash or attr_hash > max_hash:
                continue
            
            if i >= len(words):
                break
            
            combined = words[i]
            i += 1
            
            record_num = combined // 512
            slot_offset = combined % 512
            
            self.attribute_index[attr_hash] = (record_num, slot_offset)
            attr_count += 1
        
        print(f"  âœ“ è§£æåˆ° {attr_count} ä¸ªå±æ€§ç´¢å¼•")
        return attr_count
    
    def parse_atgtdf_section(self, words: List[int]):
        """è§£æ ATGTDF å±æ€§å®šä¹‰æ®µ"""
        print("\nğŸ” è§£æ ATGTDF (å±æ€§å®šä¹‰æ®µ)...")
        
        i = 0
        attr_count = 0
        min_hash = 531442
        max_hash = 387951929
        
        while i < len(words) - 2:
            attr_hash = words[i]
            i += 1
            
            # èŒƒå›´æ£€æŸ¥
            if attr_hash < min_hash or attr_hash > max_hash:
                continue
            
            if i >= len(words) - 1:
                break
            
            data_type = words[i]
            i += 1
            default_flag = words[i]
            i += 1
            
            # è§£æé»˜è®¤å€¼
            default_value = None
            if default_flag == 2:  # æœ‰é»˜è®¤å€¼
                if data_type == 4:  # TEXT ç±»å‹
                    if i < len(words):
                        text_length = words[i]
                        i += 1
                        text_data = words[i:i+text_length]
                        i += text_length
                        default_value = self.decode_27_base(text_data)
                else:  # æ ‡é‡ç±»å‹
                    if i < len(words):
                        default_value = words[i]
                        i += 1
            
            self.attribute_definitions[attr_hash] = {
                'hash': attr_hash,
                'data_type': data_type,
                'default_flag': default_flag,
                'default_value': default_value
            }
            attr_count += 1
        
        print(f"  âœ“ è§£æåˆ° {attr_count} ä¸ªå±æ€§å®šä¹‰")
        return attr_count
    
    def analyze_noun_types(self):
        """ä»å±æ€§å®šä¹‰ä¸­åˆ†æ Noun ç±»å‹"""
        print("\nğŸ” åˆ†æ Noun ç±»å‹...")
        
        # Noun ç±»å‹çš„ç‰¹å¾ï¼š
        # 1. hash å€¼é€šå¸¸åœ¨ç‰¹å®šèŒƒå›´
        # 2. æœ‰ç‰¹å®šçš„å±æ€§é›†åˆï¼ˆå¦‚ NAME, OWNER ç­‰ï¼‰
        
        # æ ¹æ®å·²çŸ¥çš„ Noun hash å€¼è¯†åˆ«
        known_noun_hashes = [
            564937,   # WORL
            631900,   # SITE  
            724361,   # ZONE
            907462,   # EQUI
            958465,   # PIPE
            640493,   # ELBO
            # ... æ›´å¤š
        ]
        
        noun_count = 0
        for noun_hash in known_noun_hashes:
            if noun_hash in self.attribute_index or noun_hash in self.attribute_definitions:
                self.noun_definitions[noun_hash] = {
                    'hash': noun_hash,
                    'identified': True
                }
                noun_count += 1
        
        print(f"  âœ“ è¯†åˆ«åˆ° {noun_count} ä¸ªå·²çŸ¥ Noun ç±»å‹")
        return noun_count
    
    def extract_all_data(self, output_path: str):
        """æå–æ‰€æœ‰æ•°æ®"""
        print("\n" + "="*70)
        print(" "*15 + "å®Œæ•´è§£æ attlib.dat")
        print("="*70)
        
        if not self.attlib_path.exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {self.attlib_path}")
            return None
        
        file_size = self.attlib_path.stat().st_size
        print(f"\nğŸ“ æ–‡ä»¶: {self.attlib_path}")
        print(f"ğŸ“ å¤§å°: {file_size:,} å­—èŠ‚ ({file_size/1024/1024:.2f} MB)")
        
        with open(self.attlib_path, 'rb') as f:
            # 1. è¯»å–æ–‡ä»¶å¤´
            print("\nğŸ“– Step 1: è¯»å–æ–‡ä»¶å¤´...")
            header = self.read_file_header(f)
            
            # 2. è¯»å–æ®µæŒ‡é’ˆ
            print("\nğŸ“– Step 2: è¯»å–æ®µæŒ‡é’ˆè¡¨...")
            self.section_pointers = self.read_section_pointers(f)
            
            if len(self.section_pointers) < 2:
                print("âŒ æ®µæŒ‡é’ˆä¸è¶³")
                return None
            
            # 3. è§£æå„ä¸ªæ®µ
            print("\nğŸ“– Step 3: è§£ææ•°æ®æ®µ...")
            
            # ATGTIX - å±æ€§ç´¢å¼•æ®µ
            atgtix_words = self.parse_section(f, self.section_pointers[0], "ATGTIX")
            self.parse_atgtix_section(atgtix_words)
            
            # ATGTDF - å±æ€§å®šä¹‰æ®µ
            atgtdf_words = self.parse_section(f, self.section_pointers[1], "ATGTDF")
            self.parse_atgtdf_section(atgtdf_words)
            
            # å…¶ä»–æ®µ
            for i in range(2, min(len(self.section_pointers), 8)):
                section_words = self.parse_section(f, self.section_pointers[i], f"æ®µ{i+1}")
                print(f"  æ®µ {i+1}: {len(section_words)} ä¸ªå­—")
            
            # 4. åˆ†æ Noun ç±»å‹
            print("\nğŸ“– Step 4: åˆ†æ Noun ç±»å‹...")
            self.analyze_noun_types()
            
            # 5. ç”Ÿæˆè¾“å‡º
            print("\nğŸ“– Step 5: ç”Ÿæˆè¾“å‡º...")
            output_data = {
                "version": "3.0",
                "source": "attlib.dat å®Œæ•´è§£æ (ä¸ä¾èµ– JSON)",
                "description": "ä» attlib.dat äºŒè¿›åˆ¶æ–‡ä»¶ç›´æ¥æå–çš„æ‰€æœ‰æ•°æ®",
                "file_info": {
                    "path": str(self.attlib_path),
                    "size_bytes": file_size,
                    "sections_count": len(self.section_pointers)
                },
                "statistics": {
                    "attribute_index_count": len(self.attribute_index),
                    "attribute_definitions_count": len(self.attribute_definitions),
                    "noun_types_count": len(self.noun_definitions),
                },
                "attribute_index": {
                    str(k): {"record_num": v[0], "slot_offset": v[1]}
                    for k, v in list(self.attribute_index.items())[:100]  # ç¤ºä¾‹ï¼šå‰100ä¸ª
                },
                "attribute_definitions": {
                    str(k): v
                    for k, v in list(self.attribute_definitions.items())[:100]  # ç¤ºä¾‹ï¼šå‰100ä¸ª
                },
                "noun_definitions": {
                    str(k): v
                    for k, v in self.noun_definitions.items()
                },
            }
            
            # ä¿å­˜æ–‡ä»¶
            output_file = Path(output_path)
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, indent=2, ensure_ascii=False)
            
            print(f"\nâœ… æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
            print(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
            for key, value in output_data['statistics'].items():
                print(f"   - {key}: {value}")
            
            return output_data


def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    attlib_path = "/Volumes/DPC/work/plant-code/rs-core/data/attlib.dat"
    output_path = "/Volumes/DPC/work/plant-code/rs-core/attlib_complete_parsed.json"
    
    if len(sys.argv) > 1:
        attlib_path = sys.argv[1]
    if len(sys.argv) > 2:
        output_path = sys.argv[2]
    
    parser = AttlibCompleteParser(attlib_path)
    result = parser.extract_all_data(output_path)
    
    if result:
        print("\n" + "="*70)
        print(" "*25 + "âœ¨ è§£æå®Œæˆï¼")
        print("="*70)
    else:
        print("\nâŒ è§£æå¤±è´¥")


if __name__ == "__main__":
    main()
