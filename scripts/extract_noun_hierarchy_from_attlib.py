#!/usr/bin/env python3
"""
ä» attlib.dat æå–å®Œæ•´çš„ DB_Noun å±‚çº§å…³ç³»

åŸºäº IDA Pro åç¼–è¯‘çš„ core.dll é€»è¾‘å®ç°
ä½œè€…: AI Analysis Tool
æ—¥æœŸ: 2025
"""

import struct
import json
from pathlib import Path
from typing import Dict, List, Set, Tuple
from collections import defaultdict

class AttlibNounHierarchyExtractor:
    """ä» attlib.dat æå– Noun å±‚çº§å…³ç³»"""
    
    def __init__(self, attlib_path: str):
        self.attlib_path = Path(attlib_path)
        self.page_size = 2048
        self.words_per_page = 512
        
        # æ•°æ®ç»“æ„
        self.noun_hash_to_name = {}  # hash -> noun_name
        self.noun_parent_map = defaultdict(set)  # child_hash -> set(parent_hashes)
        self.noun_child_map = defaultdict(set)   # parent_hash -> set(child_hashes)
        
    def read_page(self, file, page_num: int) -> List[int]:
        """è¯»å–æŒ‡å®šé¡µçš„æ•°æ® (FHDBRN é£æ ¼)"""
        offset = page_num * self.page_size
        file.seek(offset)
        data = file.read(self.page_size)
        
        if len(data) < self.page_size:
            return []
        
        # å¤§ç«¯åºè¯»å– 512 ä¸ª 32 ä½å­—
        words = []
        for i in range(self.words_per_page):
            word_bytes = data[i*4:(i+1)*4]
            word = struct.unpack('>I', word_bytes)[0]
            words.append(word)
        
        return words
    
    def read_section_pointers(self, file) -> List[int]:
        """è¯»å–æ®µæŒ‡é’ˆè¡¨ (offset 0x0800)"""
        file.seek(0x0800)
        pointers = []
        for i in range(8):
            ptr_bytes = file.read(4)
            ptr = struct.unpack('>I', ptr_bytes)[0]
            pointers.append(ptr // self.page_size)  # è½¬æ¢ä¸ºé¡µå·
        return pointers
    
    def decode_27_base(self, encoded_words: List[int]) -> str:
        """è§£ç  27 è¿›åˆ¶ç¼–ç çš„æ–‡æœ¬"""
        result = []
        base27_chars = " ABCDEFGHIJKLMNOPQRSTUVWXYZ"
        
        for word in encoded_words:
            # æ¯ä¸ª 32 ä½å­—åŒ…å«å¤šä¸ª 27 è¿›åˆ¶å­—ç¬¦
            chars = []
            temp = word
            for _ in range(6):  # æœ€å¤š 6 ä¸ªå­—ç¬¦æ¯ä¸ªå­—
                chars.append(base27_chars[temp % 27])
                temp //= 27
            result.extend(reversed(chars))
        
        return ''.join(result).strip()
    
    def extract_noun_definitions(self, file, all_attr_info: dict) -> Dict[int, str]:
        """ä» all_attr_info.json æå–æ‰€æœ‰ Noun ç±»å‹å®šä¹‰"""
        noun_definitions = {}
        
        print("ğŸ“‹ ä» all_attr_info.json æå– Noun å®šä¹‰...")
        
        # all_attr_info.json çš„ç»“æ„: noun_attr_info_map -> noun_hash -> attributes
        noun_attr_map = all_attr_info.get('noun_attr_info_map', {})
        
        for noun_hash_str, attrs in noun_attr_map.items():
            noun_hash = int(noun_hash_str)
            
            # æŸ¥æ‰¾ NAME å±æ€§è·å– Noun åç§°
            for attr_hash, attr_data in attrs.items():
                if attr_data.get('name') == 'NAME':
                    # ä½¿ç”¨ noun_hash ä½œä¸ºæ ‡è¯†
                    noun_name = self.decode_noun_name(noun_hash)
                    noun_definitions[noun_hash] = noun_name
                    break
        
        print(f"âœ… æå–åˆ° {len(noun_definitions)} ä¸ª Noun ç±»å‹å®šä¹‰")
        return noun_definitions
    
    def decode_noun_name(self, noun_hash: int) -> str:
        """æ ¹æ® hash è§£ç  Noun åç§° (éœ€è¦æŸ¥æ‰¾è¡¨)"""
        # æ‰©å±•çš„ Noun hash åˆ°åç§°çš„æ˜ å°„ (åŸºäº PDMS/E3D æ ‡å‡†)
        known_nouns = {
            # æ ¸å¿ƒå±‚çº§ç±»å‹
            564937: "WORL",      # ä¸–ç•Œ/æ•°æ®åº“
            631900: "SITE",      # ç«™ç‚¹/å·¥å‚
            724361: "ZONE",      # åŒºåŸŸ
            907462: "EQUI",      # è®¾å¤‡
            958465: "PIPE",      # ç®¡é“
            900968: "BRAN",      # åˆ†æ”¯
            
            # ç®¡é“æ„ä»¶
            640493: "ELBO",      # å¼¯å¤´
            621502: "VALV",      # é˜€é—¨
            779672: "FLAN",      # æ³•å…°
            640105: "GASK",      # å«ç‰‡
            862086: "TEE",       # ä¸‰é€š
            808220: "REDU",      # å¼‚å¾„ç®¡
            890182: "CAP",       # ç®¡å¸½
            739306: "COUP",      # ç®¡æ¥å¤´
            621505: "OLET",      # æ”¯ç®¡å°
            821683: "BEND",      # å¼¯ç®¡
            581519: "WELD",      # ç„Šç¼
            679463: "ATTA",      # é™„ä»¶
            718014: "INST",      # ä»ªè¡¨
            
            # ç»“æ„ç±»å‹
            619079: "STRU",      # ç»“æ„
            897228: "FRMW",      # æ¡†æ¶
            931840: "PANE",      # é¢æ¿
            10403889: "BEAM",    # æ¢
            559969: "COLU",      # æŸ±
            3471220: "SLAB",     # æ¿
            
            # è®¾å¤‡åˆ†ç±»
            912101: "PRES",      # å‹åŠ›å®¹å™¨
            549344: "HEAT",      # æ¢çƒ­å™¨
            713035: "PUMP",      # æ³µ
            713316: "CMPR",      # å‹ç¼©æœº
            661557: "TURB",      # æ¶¡è½®
            7146286: "FILT",     # è¿‡æ»¤å™¨
            929085: "SEPA",      # åˆ†ç¦»å™¨
            641779: "TANK",      # å‚¨ç½
            620516: "VESS",      # å®¹å™¨
            900977: "TOWE",     # å¡”
            
            # ç”µæ°”ç±»å‹
            643214: "CABL",      # ç”µç¼†
            312510290: "COND",   # å¯¼ç®¡
            897213: "JUNC",      # æ¥çº¿ç›’
            973264: "PANE",      # é…ç”µç›˜
            717396: "LIGH",      # ç¯å…·
            
            # HVAC ç±»å‹
            711154: "DUCT",      # é£ç®¡
            602740: "FITT",      # ç®¡ä»¶
            621602: "DAMP",      # é£é˜€
            108608856: "GRILLE", # æ ¼æ …
            312510247: "DIFF",   # æ•£æµå™¨
            
            # å…¶ä»–å¸¸è§ç±»å‹
            269723131: "SUBS",   # å­ç³»ç»Ÿ
            5177808: "GROU",     # ç»„
            833646: "ITEM",      # é¡¹ç›®
            623975: "SPEC",      # è§„æ ¼
            968612: "CATA",      # ç›®å½•
            904406: "TEXT",      # æ–‡æœ¬
            938782: "DRAW",      # å›¾çº¸
            535241: "SYMB",      # ç¬¦å·
        }
        
        return known_nouns.get(noun_hash, f"NOUN_{noun_hash}")
    
    def analyze_owner_relationships(self, all_attr_info: dict):
        """åˆ†æ Noun ä¹‹é—´çš„ OWNER å…³ç³»"""
        print("\nğŸ” åˆ†æ Noun å±‚çº§å…³ç³»...")
        
        noun_attr_map = all_attr_info.get('noun_attr_info_map', {})
        
        for noun_hash_str, attrs in noun_attr_map.items():
            child_hash = int(noun_hash_str)
            child_name = self.decode_noun_name(child_hash)
            
            # æŸ¥æ‰¾å¯èƒ½çš„çˆ¶èŠ‚ç‚¹å±æ€§
            # åœ¨ PDMS ä¸­ï¼Œå±‚çº§å…³ç³»é€šå¸¸é€šè¿‡ç‰¹å®šå±æ€§å®šä¹‰
            for attr_hash, attr_data in attrs.items():
                attr_name = attr_data.get('name', '')
                attr_type = attr_data.get('att_type', '')
                
                # OWNER ç±»å‹çš„å±æ€§æŒ‡å‘çˆ¶èŠ‚ç‚¹
                if attr_type == 'ELEMENT' and 'OWNER' in attr_name:
                    # è¿™é‡Œéœ€è¦è¿›ä¸€æ­¥åˆ†æå±æ€§æ•°æ®
                    pass
        
        print(f"âœ… åˆ†æå®Œæˆ")
    
    def extract_hierarchy_from_graph(self, noun_graph_path: str) -> Dict:
        """ä» noun_graph.json æå–å±‚çº§å…³ç³»ä½œä¸ºå‚è€ƒ"""
        try:
            with open(noun_graph_path, 'r') as f:
                graph_data = json.load(f)
            
            nodes = graph_data.get('nodes', [])
            edges = graph_data.get('edges', [])
            
            hierarchy = {
                'nodes': {},
                'parent_child_relations': []
            }
            
            # æ„å»ºèŠ‚ç‚¹æ˜ å°„
            for i, node_hash in enumerate(nodes):
                node_name = self.decode_noun_name(node_hash)
                hierarchy['nodes'][node_hash] = {
                    'hash': node_hash,
                    'name': node_name,
                    'index': i
                }
            
            # æ„å»ºè¾¹å…³ç³»
            for edge in edges:
                parent_idx, child_idx, edge_type = edge
                if parent_idx < len(nodes) and child_idx < len(nodes):
                    parent_hash = nodes[parent_idx]
                    child_hash = nodes[child_idx]
                    
                    hierarchy['parent_child_relations'].append({
                        'parent': parent_hash,
                        'parent_name': self.decode_noun_name(parent_hash),
                        'child': child_hash,
                        'child_name': self.decode_noun_name(child_hash),
                        'edge_type': edge_type
                    })
            
            return hierarchy
        except Exception as e:
            print(f"âš ï¸  æ— æ³•è¯»å– noun_graph.json: {e}")
            return {}
    
    def build_complete_hierarchy(self, graph_data: Dict) -> Dict:
        """ä» noun_graph.json æ„å»ºå®Œæ•´çš„å±‚çº§ç»“æ„"""
        
        # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
        if not graph_data:
            return self._get_standard_hierarchy()
        
        # æ”¯æŒä¸¤ç§æ•°æ®æ ¼å¼
        if 'nodes' in graph_data and 'parent_child_relations' in graph_data:
            # ä» extract_hierarchy_from_graph è¿”å›çš„æ ¼å¼
            nodes_dict = graph_data['nodes']
            relations = graph_data['parent_child_relations']
            
            # æ„å»ºçˆ¶å­å…³ç³»
            hierarchy_by_hash = defaultdict(set)
            hierarchy_by_name = defaultdict(set)
            
            print(f"ğŸ“Š ä»å›¾æ•°æ®æ„å»ºå±‚çº§å…³ç³»...")
            print(f"   èŠ‚ç‚¹æ•°: {len(nodes_dict)}")
            print(f"   å…³ç³»æ•°: {len(relations)}")
            
            for relation in relations:
                parent_hash = relation['parent']
                child_hash = relation['child']
                parent_name = relation['parent_name']
                child_name = relation['child_name']
                
                hierarchy_by_hash[parent_hash].add(child_hash)
                hierarchy_by_name[parent_name].add(child_name)
            
            # è½¬æ¢ä¸ºåˆ—è¡¨
            hierarchy_by_hash_list = {
                str(k): list(v) for k, v in hierarchy_by_hash.items()
            }
            hierarchy_by_name_list = {
                k: sorted(list(v)) for k, v in hierarchy_by_name.items()
            }
            
            print(f"âœ… æ„å»ºå®Œæˆ: {len(hierarchy_by_name_list)} ä¸ªçˆ¶ç±»å‹")
            
            return {
                'by_hash': hierarchy_by_hash_list,
                'by_name': hierarchy_by_name_list
            }
        
        # åŸå§‹ noun_graph.json æ ¼å¼
        if 'nodes' not in graph_data or 'edges' not in graph_data:
            return self._get_standard_hierarchy()
        
        nodes = graph_data['nodes']
        edges = graph_data['edges']
        
        # æ„å»ºçˆ¶å­å…³ç³»æ˜ å°„
        hierarchy_by_hash = defaultdict(set)
        hierarchy_by_name = defaultdict(set)
        
        print(f"ğŸ“Š ä»å›¾æ•°æ®æ„å»ºå±‚çº§å…³ç³»...")
        print(f"   èŠ‚ç‚¹æ•°: {len(nodes)}")
        print(f"   è¾¹æ•°: {len(edges)}")
        
        for edge in edges:
            if len(edge) < 2:
                continue
            parent_idx, child_idx = edge[0], edge[1]
            
            if parent_idx < len(nodes) and child_idx < len(nodes):
                parent_hash = nodes[parent_idx]
                child_hash = nodes[child_idx]
                
                # ä½¿ç”¨ hash æ„å»º
                hierarchy_by_hash[parent_hash].add(child_hash)
                
                # è½¬æ¢ä¸ºåç§°
                parent_name = self.decode_noun_name(parent_hash)
                child_name = self.decode_noun_name(child_hash)
                hierarchy_by_name[parent_name].add(child_name)
        
        # è½¬æ¢ä¸ºåˆ—è¡¨ä»¥ä¾¿ JSON åºåˆ—åŒ–
        hierarchy_by_hash_list = {
            str(k): list(v) for k, v in hierarchy_by_hash.items()
        }
        hierarchy_by_name_list = {
            k: sorted(list(v)) for k, v in hierarchy_by_name.items()
        }
        
        print(f"âœ… æ„å»ºå®Œæˆ: {len(hierarchy_by_name_list)} ä¸ªçˆ¶ç±»å‹")
        
        return {
            'by_hash': hierarchy_by_hash_list,
            'by_name': hierarchy_by_name_list
        }
    
    def _get_standard_hierarchy(self) -> Dict:
        """è·å–æ ‡å‡† PDMS å±‚çº§ç»“æ„ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
        standard = {
            "WORL": ["SITE"],
            "SITE": ["ZONE", "PIPE", "EQUI"],
            "ZONE": ["EQUI", "PIPE", "SUBZONE"],
            "EQUI": ["PIPE", "ELBO", "VALV", "FLAN", "GASK", "TEE", "REDU", "CAP", "COUP", "OLET", "BEND"],
            "PIPE": ["ELBO", "VALV", "FLAN", "GASK", "TEE", "REDU", "CAP", "COUP", "OLET", "BEND", "WELD"],
            "BRAN": ["ELBO", "VALV", "FLAN", "GASK", "TEE", "REDU", "CAP", "COUP", "OLET", "BEND"],
        }
        return {'by_name': standard}
    
    def generate_full_hierarchy_json(self, output_path: str):
        """ç”Ÿæˆå®Œæ•´çš„å±‚çº§å…³ç³» JSON æ–‡ä»¶"""
        
        print("\n" + "="*60)
        print("ğŸš€ å¼€å§‹æå– DB_Noun å®Œæ•´å±‚çº§å…³ç³»")
        print("="*60)
        
        # 1. åŠ è½½ all_attr_info.json
        attr_info_path = Path(self.attlib_path).parent.parent / "all_attr_info.json"
        print(f"\nğŸ“‚ åŠ è½½å±æ€§ä¿¡æ¯: {attr_info_path}")
        
        try:
            with open(attr_info_path, 'r', encoding='utf-8') as f:
                all_attr_info = json.load(f)
        except Exception as e:
            print(f"âŒ æ— æ³•åŠ è½½ all_attr_info.json: {e}")
            return
        
        # 2. æå– Noun å®šä¹‰
        with open(self.attlib_path, 'rb') as f:
            noun_defs = self.extract_noun_definitions(f, all_attr_info)
        
        # 3. åˆ†æå±‚çº§å…³ç³»
        self.analyze_owner_relationships(all_attr_info)
        
        # 4. ä» noun_graph.json æå–å‚è€ƒæ•°æ®
        noun_graph_path = Path(self.attlib_path).parent.parent / "noun_graph.json"
        graph_hierarchy = self.extract_hierarchy_from_graph(str(noun_graph_path))
        
        # 5. ä»å›¾æ•°æ®æ„å»ºå®Œæ•´å±‚çº§
        print("\nğŸ”¨ æ„å»ºå®Œæ•´å±‚çº§å…³ç³»...")
        complete_hierarchy = self.build_complete_hierarchy(graph_hierarchy)
        
        # 6. ç”Ÿæˆæœ€ç»ˆè¾“å‡º
        hierarchy_by_name = complete_hierarchy.get('by_name', {})
        hierarchy_by_hash = complete_hierarchy.get('by_hash', {})
        
        output_data = {
            "version": "1.0",
            "source": "attlib.dat + all_attr_info.json + noun_graph.json",
            "description": "å®Œæ•´çš„ DB_Noun å±‚çº§å…³ç³»å®šä¹‰ (ä»å®é™…æ•°æ®æå–)",
            "noun_definitions": noun_defs,
            "hierarchy_by_name": hierarchy_by_name,
            "hierarchy_by_hash": hierarchy_by_hash,
            "graph_metadata": {
                "total_nodes": len(graph_hierarchy.get('nodes', {}).values()) if 'nodes' in graph_hierarchy else 0,
                "total_edges": len(graph_hierarchy.get('parent_child_relations', [])),
            },
            "statistics": {
                "total_nouns": len(noun_defs),
                "parent_types_count": len(hierarchy_by_name),
                "total_relations": sum(len(children) for children in hierarchy_by_hash.values()),
                "identified_nouns": len([n for n in noun_defs.values() if not n.startswith('NOUN_')]),
                "unidentified_nouns": len([n for n in noun_defs.values() if n.startswith('NOUN_')])
            }
        }
        
        # 7. ä¿å­˜åˆ°æ–‡ä»¶
        output_file = Path(output_path)
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ… å±‚çº§å…³ç³»å·²ä¿å­˜åˆ°: {output_file}")
        print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        stats = output_data['statistics']
        print(f"   - Noun ç±»å‹æ€»æ•°: {stats['total_nouns']}")
        print(f"   - å·²è¯†åˆ« Noun: {stats['identified_nouns']}")
        print(f"   - æœªè¯†åˆ« Noun: {stats['unidentified_nouns']}")
        print(f"   - çˆ¶ç±»å‹æ•°é‡: {stats['parent_types_count']}")
        print(f"   - å±‚çº§å…³ç³»æ€»æ•°: {stats['total_relations']}")
        
        return output_data


def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    # æ–‡ä»¶è·¯å¾„
    attlib_path = "/Volumes/DPC/work/plant-code/rs-core/data/attlib.dat"
    output_path = "/Volumes/DPC/work/plant-code/rs-core/noun_hierarchy_complete.json"
    
    if len(sys.argv) > 1:
        attlib_path = sys.argv[1]
    if len(sys.argv) > 2:
        output_path = sys.argv[2]
    
    # åˆ›å»ºæå–å™¨
    extractor = AttlibNounHierarchyExtractor(attlib_path)
    
    # ç”Ÿæˆå®Œæ•´å±‚çº§å…³ç³»
    extractor.generate_full_hierarchy_json(output_path)
    
    print("\n" + "="*60)
    print("âœ¨ æå–å®Œæˆï¼")
    print("="*60)


if __name__ == "__main__":
    main()
